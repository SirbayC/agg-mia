This repository contains the codes and artifacts for the paper, [TraWiC: Trained Without My Consent](https://arxiv.org/abs/2402.09299). The adaptation focuses on cleaning up files to strip the codebase down to a basic end-to-end MIA flow.

`TraWiC` is a tool designed for dataset inclusion detection in the training dataset of large language models trained on code using membership inference attacks.

This version of the repo has been stripped down to the core SantaCoder + Random Forest flow (syntactic threshold: 100, semantic threshold: 20).

---

# Project Structure

```
src/
├── main_santacoder.py         # Step 3 - Run model inference
├── inspector_train.py         # Step 5 - Train RF classifier
├── inspector_test.py          # Step 6 - Evaluate RF classifier
├── run_tests.py               # Sanity check
├── checker/
│   ├── checker.py             # Code similarity checker
│   └── colator.py             # Input collation utilities
├── models/
│   ├── santa.py               # SantaCoder model wrapper
│   └── model.py               # Base model interface
├── data/
│   ├── dataset.py             # TheStack downloader
│   ├── dataset_builder.py     # Step 4 - Build RF training data
│   ├── download_repos.py      # GitHub repo downloader
│   └── discover_repos.py      # Repo discovery utilities
└── utils/
    └── token_writer.py        # Token writing utilities
data/
├── repos/                     # Cloned GitHub repos (Step 2b)
├── the_stack/python/          # TheStack scripts (Step 2a)
├── repos_fim.json             # List of GitHub repos to clone
└── discard_fim.json           # Repos to exclude
rf_data/
└── syn100_sem20/
    ├── train.csv              # Generated by Step 4
    └── test.csv               # Generated by Step 4
run_results/                   # Model inference outputs (Step 3)
    ├── results.jsonl
    ├── processed_tokens.txt
    ├── too_many_tokens.txt
    └── assert_errors.txt
```

---

# How to Run

## 1 - Setup

Python 3.10 is required. Create and activate a virtual environment, then install dependencies:

```bash
pip install -r requirements.txt
```

Login to HuggingFace (required to download SantaCoder and TheStack):

```bash
huggingface-cli login
```

## 2 - Download The Dataset

**TheStack** (used for positive samples — trained on):
```bash
python src/data/dataset.py
```
This saves Python scripts to `data/the_stack/python/`. The dataset is large — ensure you have enough disk space.

**GitHub repos** (scripts used for negative samples — not trained on):
```bash
python src/data/download_repos.py
```
This clones the repositories listed in `data/repos_fim.json` into `data/repos/`.

## 3 - Sanity Check

```bash
python src/run_tests.py
```

## 4 - Run Model Inference

Run SantaCoder over the dataset to generate similarity features. Run from the **project root**:

```bash
python src/main_santacoder.py
```

By default, outputs are saved to `run_results/results.jsonl`.

Optional arguments:
- `--output_dir` — custom output directory (default: `run_results`)
- `--dataset_path` — path to the dataset (default: `data`)
- `--sorted` — process files in sorted order (default: `False`)
- `--limit_per_class` — max files per class: TheStack and repos (default: `50`)

Example with custom output:
```bash
python src/main_santacoder.py --output_dir="./my_run"
```

## 5 - Build the Classification Dataset

```bash
python src/data/dataset_builder.py
```

This reads the model outputs and builds `rf_data/syn100_sem20/train.csv` and `test.csv`.

Optional arguments:
- `--input_dir` — directory containing `results.jsonl` (default: `run_results`)
- `--output_dir` — directory to save rf_data outputs (default: `rf_data/syn{syn}_sem{sem}`)
- `--syntactic_threshold` — syntactic similarity threshold (default: `100`)
- `--semantic_threshold` — semantic similarity threshold (default: `20`)

Example with custom directories:
```bash
python src/data/dataset_builder.py --input_dir="./my_run" --output_dir="./my_run/rf_data"
```

## 6 - Train the Classifier

```bash
python src/inspector_train.py
```

This:
- Trains a Random Forest classifier with 5-fold grid search
- Evaluates on a held-out test split
- Saves the model to `rf_model__syn100_sem20.sav`
- Generates feature importance, distribution, and correlation plots

Optional arguments:
- `--input_dir` — directory containing `train.csv` (default: `rf_data/syn{syn}_sem{sem}`)
- `--output_dir` — directory to save model and plots (default: current directory)
- `--syntactic_threshold` — syntactic similarity threshold (default: `100`)
- `--semantic_threshold` — semantic similarity threshold (default: `20`)
- `--visualisation` — generate plots (default: `True`)

Example with custom directories:
```bash
python src/inspector_train.py --input_dir="./my_run/rf_data" --output_dir="./my_run"
```

## 7 - Evaluate the Classifier

```bash
python src/inspector_test.py
```

This loads the saved model and evaluates it at both **file level** and **repository level** (40% and 60% thresholds).

Optional arguments:
- `--input_dir` — directory containing `test.csv` (default: `rf_data/syn{syn}_sem{sem}`)
- `--model_dir` — directory containing trained model (default: current directory)
- `--output_dir` — directory to save results (default: current directory)
- `--syntactic_threshold` — syntactic similarity threshold (default: `100`)
- `--semantic_threshold` — semantic similarity threshold (default: `20`)

Results are saved to:
- `clf_rf.csv` — summary metrics
- `inspector_test_file_level_rf__syn100_sem20.csv`
- `inspector_test_repo_level_0.4_rf__syn100_sem20.csv`
- `inspector_test_repo_level_0.6_rf__syn100_sem20.csv`

Example with custom directories:
```bash
python src/inspector_test.py --input_dir="./my_run/rf_data" --model_dir="./my_run" --output_dir="./my_run"
```

---

# How to Cite

```
@misc{majdinasab2024trained,
      title={Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code},
      author={Vahid Majdinasab and Amin Nikanjam and Foutse Khomh},
      year={2024},
      eprint={2402.09299},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
```
